algorithm:
  class: IPL_IQL
  beta: 0.3333
  expectile: 0.75
  reward_reg: 0.5
  reg_replay_weight: 0.5
  actor_replay_weight: 0.5
  value_replay_weight: 0.5
  tau: 0.005
  max_exp_clip: 100.0
  discount: 0.99
  target_freq: 1

checkpoint: null
seed: 0
name: default
debug: false
device: null
wandb:
  activate: false
  entity: null
  project: null

env: pen-cloned-v1
env_kwargs:
env_wrapper:
env_wrapper_kwargs:

optim:
  default:
    class: Adam
    lr: 0.0003

network:
  actor:
    class: SquashedGaussianActor
    hidden_dims: [256, 256, 256]
    reparameterize: false
    conditioned_logstd: false
    logstd_min: -5
    logstd_max: 2
  critic:
    class: Critic
    ensemble_size: 2
    hidden_dims: [256, 256, 256]
  value:
    class: Critic
    ensemble_size: 1
    hidden_dims: [256, 256, 256]

dataset:
  - class: IPLComparisonOfflineDataset
    env: pen-cloned-v1
    batch_size: 8
    mode: human
    segment_length: null
  - class: D4RLOfflineDataset
    env: pen-cloned-v1
    batch_size: 256
    mode: transition
    reward_normalize: true

dataloader:
  num_workers: 0  # use the main thread to sample data
  batch_size: null  # do not merge the data along batch axis

trainer:
  env_freq: null
  total_steps: 500000
  log_freq: 500
  profile_freq: 500
  eval_freq: 5000

eval:
  function: eval_offline
  num_ep: 10
  deterministic: true

schedulers:

processor: null
