algorithm:
  class: BTIQL
  beta: 0.3333
  expectile: 0.7
  max_exp_clip: 100.0
  reward_reg: 0.0
  rm_label: true

checkpoint: null
seed: 0
name: default
debug: false
device: null
wandb:
  activate: false
  entity: null
  project: null

env: Walker2d-v3
env_kwargs:
env_wrapper: MujocoParamOverWrite
env_wrapper_kwargs:
  overwrite_args:
    gravity: 1.5  # gravity-150
  do_scale: True

eval_env: <env>
eval_env_kwargs:
eval_env_wrapper: MujocoParamOverWrite
eval_env_wrapper_kwargs:
  overwrite_args:
    gravity: 0.1  # gravity-10
  do_scale: True

optim:
  default:
    class: Adam
    lr: 0.0003

network:
  reward:
    class: EnsembleMLP
    ensemble_size: 1
    hidden_dims: [256, 256]
    reward_act: sigmoid
  actor:
    class: SquashedGaussianActor
    hidden_dims: [256, 256]
    reparameterize: false
    conditioned_logstd: false
    logstd_min: -5
    logstd_max: 2
  critic:
    class: Critic
    ensemble_size: 2
    hidden_dims: [256, 256]
  value:
    class: Critic
    ensemble_size: 1
    hidden_dims: [256, 256]

rm_dataset:
  - class: RPLComparisonDataset
    env: <env>
    batch_size: 8
    segment_length: null
    label_key: rl_sum
    variant: gravity-150
    capacity: 500
rm_dataloader:
  num_workers: 2
  batch_size: null

rl_dataset:
  - class: RPLOfflineDataset
    env: <env>
    batch_size: 16
    variant: gravity-10
    capacity: 5000
rl_dataloader:
  num_workers: 2
  batch_size: null

trainer:
  env_freq: null
  rm_label: true
  rm_steps: 50000
  rl_steps: 1000000
  log_freq: 500
  profile_freq: 500
  eval_freq: 5000

rm_eval:
  function: eval_reward_model
  eval_dataset_kwargs:
    class: RPLComparisonDataset
    env: <env>
    batch_size: 32
    label_key: rl_sum
    variant: gravity-150
    eval: true
    capacity: 500
rl_eval:
  function: eval_offline
  num_ep: 10
  deterministic: true

schedulers:
  actor:
    class: CosineAnnealingLR
    T_max: 1000000

processor: null
